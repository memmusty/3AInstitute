<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Build - Machine Learning</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">Memunat I.</a><button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">Menu<i class="fas fa-bars"></i></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="build2.html#skills">Skills</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#activities">Activities</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Projects-->
        <section class="projects-section bg-light" id="projects">
            <div class="container">
                <div class="row align-items-center no-gutters mb-4 mb-lg-5">
                    <div class="project-text">
                        <h1>Decision Tree from Scratch - Towards Explainable AI</h1>
                        <hr class="d-none d-lg-block mb-4 ml-0" />

                        <h3>Introduction</h3>
                        <p>
                            Machine learning (ML) models are key component of any AI driven system. Given that cyber-physical systems are powered by AI, hence ML, there is a necessity as a practitioner of Applied Cybernetics to understand what goes on in machine learning models. Which makes me to ponder on these questions - are ML models really black boxes? Can they go totally out of control? What are machine learning models? and what aspect of these model might be of interest of practitioner of the NBE when exploring CPSs?
                        </p>
                        <p>
                            In my attempt to understand machine learning models and answer these questions, I tried to build a decision tree model from scratch on the <a href="http://archive.ics.uci.edu/ml/datasets/Zoo?ref=datanews.io">UCI zoo dataset</a>. This was my way of tearing the model apart and "seeing" how it work and make predictions.<br/>
                            This page provides a documentation of my journey trying to "tear ML models apart" and then build it from scratch. I will be discussing my progress, challenges, decision process and my thoughts on my approach to understanding ML models. Is it effective to build ML models from scratch? Are there other efficient ways to understanding ML models?
                            <br/>
                            The audience of this piece are the instructors of the Build course, hence the page does not go into the details about some basic ML and coding knowledge.
                        </p>

                        <h3>Achievement</h3>
                        At the end of the activities, I learned:
                        <ul>
                            <li>About different types of decision tree algorithms</li>
                            <li>About decision tree algorithms and their decision metrics</li>
                            <li>The steps involved in building ID3 decision tree classifier from scratch</li>
                            <li>How to build and built the ID3 decision tree classifier from scratch</li>
                            <li>How decision tree classifiers work</li>
                        </ul>
                        <h3>Prerequisite Knowledge and Learning resources</h3>
                        In preperation for the system analysis, I:
                        <ul>
                            <li>Read this <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">introductory tutorial</a> to decision trees, branches, split point, bias, variance, and measuring accuracy.</li>
                            <li>Read this <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-2/">introductory tutorial</a> model tuning and the bias-variance tradeoff.</li>
                            <li>Learned about different types of decision trees like ID3, C4.5, CART, C5.0 <a href="https://www.youtube.com/watch?v=LDRbO9a6XPU&app=desktop">here</a></li>
                            <li>Read this <a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf">tutorial</a> by Hal Daumé III on how to build machine learning decision tree algorithms from scratch</li>
                        </ul>
                        <h3>Key Skills</h3>
                        <ul>
                            <li>Python Programming</li>
                            <li>Algorithmic problem solving</li>
                            <li>Probability and Statistics</li>
                            <li>Data Analytics</li>
                            <li>Machine learning</li>
                            <li>Decision tree algorithm</li>
                        </ul>
                        <h3 id="activities">Activities</h3>
                        <h4>Data Analysis</h4>
                        <p>
                            In alignment with my biggest lesson from the Hal Daumé III resource that <i>the biggest and most important task in building a decision tree is knowing the best questions to ask and their order</i>. I attempted to manually determine the best questions to ask and their order when buiding a decision tree model for the <a href="http://archive.ics.uci.edu/ml/datasets/Zoo?ref=datanews.io">UCI zoo dataset</a>. 
                        </p>
                        <p>
                            I began with getting the dataset and saving it as a zoo dataframe in this <a href="https://colab.research.google.com/drive/1jbkcQnGmCo8jCG_o5BeSDr6PIz5FKAya?usp=sharing">Colab Notebook</a>, then explored the dataset for untidiness or dirtiness. 
                            With the python codes - 
                            <code>zoo.head(), zoo.shape</code>, <code>zoo.info()</code>, and <code>zoo.describe()</code>, I explored the data to check for:
                            <ul>
                                <li>missing data</li>
                                <li>incorrect variable types</li>
                                <li>anomalies in the data distribution</li>
                            </ul>    
                            I observed that the dataset is complete and clean - no missing data, nor incorrect types - also, the variables values are all binary expect for legs and type which are numerical.
                        </p>
                        <p>
                            Afterwards, I explored each of the animal types or labels to determine what features or properties are peculiar to each animal type. For instance, I fetched entries of the zoo dataset where the animals are type 1. The query matched 41 entries and interestingly, all 41 of them produce milk, have backbone, breath, are not venomous, and have no feathers. Then, I further explored each of these feature associated with type 1 animals and found out that only type1 animals produce milk. <br/>
                            In essence, <b><i>asking the question "does the animal produce milk" can determine if an animal in the dataset is type 1 or not</i></b>. The code documentation of this process can be found in the <i>Determining the Best Questions</i> section of this <a href="https://colab.research.google.com/drive/1jbkcQnGmCo8jCG_o5BeSDr6PIz5FKAya?usp=sharing">Colab Notebook</a>.
                        </p>
                        I repeated this process iteratively for the other animal types, and my observations related to their unique properties is described in the image below.
                        
                        <figure class="col-lg-7" >
                            <img class="img img-fluid" src="assets/img/zoo-animals-properties.jpg" alt="" />
                            <figcaption><em>Observed properties of the UCI Zoo dataset types</em></figcaption>
                        </figure>
                        Interestingly, when I fit the scikit learn decision tree classifier model on the dataset and visualized the result, Milk was the first question asked by the model as depicted below - meaning asking the question about Milk is the "best question" to ask on the first order. Prior to this exercise, I had assumed design tree algorithms split data by randomly selecting features and running probabilistic model on them. This made me think - <i>machine learning models are not black boxes afterall; they follow an explainable logic</i>.
                        <figure class="col-lg-9" >
                            <img class="img img-fluid" src="assets/img/scikit-learn ID3 viz.JPG" alt="" />
                            <figcaption><em>Scikit learn decision tree visualization</em></figcaption>
                        </figure>
                        <h4>Learning About Decision Tree Algorithms</h4>
                        <p>
                            Before this step, I had read about decision tree algorithms and discovered the different types of decision tree algorithms mainly ID3, C4.5, CART, C5.0 as mentioned in the prerequisite knowledge section. In this activity, I implemented an ID3 decision tree alogrithm from scratch. I decided to build an ID3 algorithm because I learned from reading the <a href="https://scikit-learn.org/stable/modules/tree.html">scikitlearn decision trees documentation</a> that the scikitlearn decision tree classifier is an implementation of the CART algorithm. I observed CART and ID3 are most popular among the algorithm types and <b>I decided to implement the ID3 alogrithm so that I can compare the prediction results of both ID3 and CART</b>.
                            <blockquote>The ID3 (Iterative Dichotomiser 3) algorithm is an algorithm that finds the independent variable that will give the highest information gain for the label or class (type variable in the Soo dataset) using a greedy approach. It does this recursively until the decision trees are grown to their maximum size.</blockquote>
                             CART also uses this <q>highest information gain</q> approach except that it uses a different metric to determine the feature with the highest information gain. <i>CART uses the gini index while ID3 uses Entropy</i>. <b>Entropy and Gini index are both measures of how impure the classification generated by splitting the dataset or subset using a given feature is</b>. Also, entropy produces higher values while Gini index values are always less than 1. 
                        </p>
                        <h4>Building ID3 Decision Tree</h4>
                        <p>
                            The code I wrote to implement the ID3 decision tree algorithm can be found in the ID3 Algorithm Implementation section of the <a href="https://colab.research.google.com/drive/1jbkcQnGmCo8jCG_o5BeSDr6PIz5FKAya?usp=sharing">Colab Notebook</a>. The code was written as a python implementation of the <a href="https://en.wikipedia.org/wiki/ID3_algorithm">ID3 pseudocode from wiki</a>, and the formulas for Entropy and Information Gain.
                            The formulas for entropy and information gain in shown in mathematical models as interpreted by me before implementation in the below images.
                            <div class="row">
                                <figure class="col-lg-5" >
                                    <img class="img img-fluid" src="assets/img/entropy ID3.JPG" alt="" />
                                    <figcaption><em>Entropy Formula</em></figcaption>
                                </figure>
                                <figure class="col-lg-5" >
                                    <img class="img img-fluid" src="assets/img/information gain ID3.JPG" alt="" />
                                    <figcaption><em>Information Gain Formula</em></figcaption>
                                </figure>
                            </div>
                            Below are the code snippets used in implementating the Entropy and Information Gain.
                            <pre>
    def entropy(col):
    """
    Calculate the entropy of a dataset. 
    It takes as parameter the column name.
    It returns the measures of impurity of the given column, the label column.
    The return type is double
    """

    #Get the counts of unique values in the column
    val_cnt = col.value_counts().sort_index()

    #Get the valuesin the column and their respective counts
    vals = val_cnt.index.tolist()
    counts = val_cnt.values.tolist()

    #Calculate column entropy
    entropy = 0
    for i in range(len(vals)):
        prob = counts[i]/np.sum(counts)
        ln = np.log2(prob)
        entropy = entropy - (prob * ln)
    return entropy
                            </pre>
                            <pre>
def Gain(data, var):
    """
    Calculates the information gain of a give dataset. 
    It takes as parameters the dataset whose information gain will be claculated,
    and the variable on which the information gain will be calculated.

    It returns the information gain and the return types is double
    """    
    #Calculate the entropy of the whole zoo dataset
    label = "type"
    data_entropy = entropy(data[label])
    
    ##Calculate the entropy of the dataset
    
    ##Get the values in the split variable and their respective counts  
    val_cnt = data[var].value_counts().sort_index()
    vals = val_cnt.index.tolist()
    counts = val_cnt.values.tolist()

    #Calculate the entropy of the sub dataset
    sub_entropy = 0
    for i in range(len(vals)):
        sub_data = data.loc[zoo[var] == vals[i]]
        sub_label = sub_data[label]
        prob = counts[i]/np.sum(counts)
        sub_entropy = sub_entropy + (prob * entropy(sub_label))

    #Calculate the information gain on the variable
    Information_Gain = data_entropy - sub_entropy
    return Information_Gain
       
                            </pre>
                            <h3>Challenges</h3>
                            <p>
                                Given my prior algorithmic problem solving and Python programming skills, the challenge with implementing the ID3 algorithm was mainly how to design the decision tree such that it will know the best questions to ask and ask them in the right order, determine when to cerate a branch, choose the best split point, etc. Trying to determine these questions manually through data analytics was not easy, however, building an algorithm that can do that recursively was almost an impossible task for me. The detailed instructions from the <a href="https://en.wikipedia.org/wiki/ID3_algorithm">ID3 wiki page</a> helped me get started with implementing some of the functions needed like the entropy and information gain, before implementating the ID3 algorithm itself. On the flip side, it made me conscious of the actions and decisions I took when I did it manually that I didn't take note of - how did I know what questions to ask as I examined each variable related to a type recursively? how did I know when to stop?
                            </p>
                            <p>
                                Afterwards, the next challenge was testing the model by fitting it to the dataset and making predictions on unlabeled data entries - only then can I compare its result with scikit-learn's. I was so excited about implementating the algorithm that I didn't realize I needed the “predict” or "fit" function to be able to test the model on data. I actually didn't know I needed to program the function too until then. To solve this, I spent hours searching for pseudocodes to implement decision tree classifiers "fit" or "predict" function online but couldn’t find it.
                            </p>
                            <p>
                                Since I couldn't find any resource that could help me with understanding and implementing the "fit" and "predict" functions from scratch, I used the code snippets for the Predict function by Bernd Klein from <a href="https://www.python-course.eu/Decision_Trees.php">here</a>. 
                            </p>
                            <p>
                                Even though I couldn’t implement the predict function on my own from scratch, being able to implement the ID3 algorithm myself is a huge step for me towards explainable CPSs and AI. It is my first baby step towards unraveling ML algorithms and figuring out how to make it more explainable to diverse audience. 
                            </p>
                            <p>Prior to this exercise, even though I have background statistic and probability knowledge, I had assumed a lot of things about how decision tree classifiers work as mentioned earlier; and this exercise has been really eye-opening and proved my assumptions wrong. I can proudly say I was able to implement a machine learning decision tree classifier from scratch. I am excited to get better at this and explore how to demystify CPSs and AI driven system to the public.</p>
                        <h3>Takeaway Lessons and Future Plans</h3>
                        <p>
                            The main lesson from this exercise is that machine learning algorithms are not really black-boxes, they are implementations of existing probabilistic and statistical rules and can be made explainable if conscious efforts are put into designing them in such a way that their algorithms "at the backend" are more accessible to those interested in it. It can also be more explainable by illustrating how the predictions are gotten like with the scikit-learn decision tree visualization. I see this graphical approach to explainability being handy when building <q>explainable</q> CPSs; and I intend to keep these in mind when designing ML models driven CPSs in the future.
                        </p>
                        <p>
                            Additionally, when I was trying to understand the formulas for entropy and information gain, I found having mathematical models that document and validate my understanding of the model was really useful. As a computer scientist, I used mathematical models alot to communicate logics but as I had spent the last few years as a software engineer in industry, I had increasingly forgotten how useful and handy they can be. This activity reminded me that modeling is important and it can be mathematical. As I build complex models for CPSs in the future, I see mathematical models as a tool that will be useful to me in documenting logics or processes and communicating them to practitioners.
                        </p>
                        <p>
                            Given my experience with implementating the ID3 model from scratch, I think it is more efficient to use existing pre-trained models that can carry out an intended ML when working on projects because it saves time and pre-trained models are usually trained with huge data which might be difficult to collate from scratch. This need to be preceeded with critical analysis of the models and the training dataset to check for biases and other sources of problems in ML models. If the existing model is not good enough, it can be tweaked to fit our needs via transfer learning and fine-tuning. Building the model from scratch should be the last option and should only be considered when the risk of the model at scale is high and transfer learning or other approaches to improve pre-trained models to fit our needs cannot eliminate all the anticipated high risks.
                        </p>

                        <h3>A fun Project - Building Custom Image Classifier from Scratch</h3>
                        <p>
                            I tried to build a custom image classifier from scratch with Teachable machine for the Aviary CPS, but to also achieve my learning goal of being able to build ML algorithms from scratch. On the first iteration, where I trained a model with only images of the object classes I was interested in, I learned that I needed more data for higher accuracy and if I am interested in just say "x" classes, I need atleast "x+1" object classes for the model to be robust. As the model will always classify given objects as belonging to the class it has the closest similarity to even if it is a wrong class.
                        </p>
                        <p>
                            As shown in the image below, I tried to build a classifier that can identify Magpies and superb parrots, so I created a model with just those 2 classes. Afterwards, I created P5.js application to serve as testing ground for the model. When I tested it on Magpie images, it correctly identified them. However, when I tested it on other images like Sterling, it misclassify them as Magpies.
                            <div class="row">
                                <figure class="col-lg-4" >
                                    <img class="img img-fluid" src="assets/img/TM1 daaset.JPG" alt="" />
                                    <figcaption><em>The classifier training set</em></figcaption>
                                </figure>
                                <figure class="col-lg-4" >
                                    <img class="img img-fluid" src="assets/img/magpie TM1.JPG" alt="" />
                                    <figcaption><em>The result of testing the model on a Magpie image</em></figcaption>
                                </figure>
                                <figure class="col-lg-4" >
                                    <img class="img img-fluid" src="assets/img/sterling TM1.JPG" alt="" />
                                    <figcaption><em>The result of testing the model on a Magpie image</em></figcaption>
                                </figure>
                            </div>
                            <b>The P5.js application can be found <a href="https://editor.p5js.org/memunati/sketches/iG1Dxus00">here</a>.</b>
                        </p>
                        <p>
                            On the next iteration, I trained the classifier with more classes than I was interested in and used approximately the same amount of images for each classes and as can be observed in the below images, the model doesn't only correctly classify Magpies, it also correctly classifies Sterlings as Birds too. 
                        </p>
                        <div class="row">
                            <figure class="col-lg-4" >
                                <img class="img img-fluid" src="assets/img/model training 2.JPG" alt="" />
                                <figcaption><em>The updated classifier training set</em></figcaption>
                            </figure>
                            <figure class="col-lg-4" >
                                <img class="img img-fluid" src="assets/img/magpie TM1.JPG" alt="" />
                                <figcaption><em>The result of testing the model on a Magpie image</em></figcaption>
                            </figure>
                            <figure class="col-lg-4" >
                                <img class="img img-fluid" src="assets/img/sterling TM2.JPG" alt="" />
                                <figcaption><em>The result of testing the model on a Magpie image</em></figcaption>
                            </figure>
                        </div>
                        <b>I intend to use about 1000 images for each class in the next iteration and tune the model's parameters like the epochs and learning rate to see what happens.</b>

                        <h4 style="margin-top: 1rem;">Cheers! and Stay Tuned...</h4>
                    </div>
                </div>
            </div>
        </section>
        <!-- Contact-->
        <section class="contact-section bg-black" id="contact">
            <div class="container">
                <div class="row">
                    <div class="col-md-4 mb-3 mb-md-0">
                        <div class="card py-4 h-100">
                            <div class="card-body text-center">
                                <i class="fas fa-globe text-primary mb-2"></i>
                                <h4 class="text-uppercase m-0">Website</h4>
                                <hr class="my-4" />
                                <div class="small text-black-50">https://memmusty.github.io/3AInstitute/</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-4 mb-3 mb-md-0">
                        <div class="card py-4 h-100">
                            <div class="card-body text-center">
                                <i class="fas fa-envelope text-primary mb-2"></i>
                                <h4 class="text-uppercase m-0">Email</h4>
                                <hr class="my-4" />
                                <div class="small text-black-50"><a href="#!">memunati@gmail.com</a></div>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-4 mb-3 mb-md-0">
                        <div class="card py-4 h-100">
                            <div class="card-body text-center">
                                <i class="fab fa-linkedin text-primary mb-2"></i>
                                <h4 class="text-uppercase m-0">LinkedIn</h4>
                                <hr class="my-4" />
                                <div class="small text-black-50"><a href="https://www.linkedin.com/in/memunat-ajoke-ibrahim/m">Memunat Ajoke Ibrahim</a></div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="social d-flex justify-content-center">
                    <a class="mx-2" href="https://www.linkedin.com/in/memunat-ajoke-ibrahim/"><i class="fab fa-linkedin"></i></a>
                    <a class="mx-2" href="https://github.com/memmusty/"><i class="fab fa-github"></i></a>
                    <a class="mx-2" href="https://twitter.com/_memunat_"><i class="fab fa-twitter"></i></a>
                    <a class="mx-2" href="https://www.facebook.com/memunatj"><i class="fab fa-facebook-f"></i></a>
                </div>
            </div>
        </section>
        <!-- Footer-->
        <footer class="footer bg-black small text-center text-white-50"><div class="container">Copyright © MemunatI. 2020</div></footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
